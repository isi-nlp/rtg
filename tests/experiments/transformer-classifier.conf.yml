environment:
  data: tests/data/dbpedia
model_args:
  src_vocab: 2500
  n_classes: 14
  enc_layers: 3
  hid_size: 128
  ff_size: 256
  n_heads: 2
  attn_bias: true
  attn_dropout: 0.1
  dropout: 0.2
  activation: relu
model_type: transformer-classifier
__parent:
  experiment: <path/to/experiment/dir>
  vocab:
    src: src            # for separate vocabs
    #tgt: tgt
  shrink: true        # shrink vocabularies and embeddings to child data
                      # specified in train_src and mono_src
  model:
    args: true          # update/overwrite the model_args of child with the parent
    ensemble: 5         # how many checkpoints of parent to ensemble, to obtain initial state

optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1
    weight_decay: 0

schedule:
  name: noam
  args:
    constant: 2
    warmup: 4000
    model_dim: 128

criterion:
  name: sparse_cross_entropy
  # no args

prep:
  max_types: 2500
  pieces: bpe
  codec_lib: nlcodec
  shared_vocab: false
  src_len: 256
  tgt_len: 100
  train_src: $data/train.text
  train_tgt: $data/train.label
  truncate: true
  valid_src: $data/valid.text
  valid_tgt: $data/valid.label
tester:
  suite:
    valid:
    - $data/valid.text
    - $data/valid.label
  ensemble: 2
  batch_size: 6000
  max_len: 256

trainer:
  batch_size: [10240, 64]  # max_toks, max_lines   -- whichever reaches first
  check_point: 500
  keep_models: 10
  steps: 20000
  sort_by: random
updated_at: '2021-07-31T19:29:42.341722'
