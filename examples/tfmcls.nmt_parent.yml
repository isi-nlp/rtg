model_args:
  src_vocab: 466
  tgt_vocab: 14
  enc_layers: 1
  hid_size: 128
  ff_size: 256
  n_heads: 2
  attn_bias: true
  attn_dropout: 0.1
  dropout: 0.2
  activation: relu
model_type: tfmcls
parent:
  experiment: runs/006-tfm-ndb-ddp
  vocab:
    src: shared            # copy shared vocab of NMT to source vocab of classifier
    #tgt: tgt              # don't copy target
  shrink: false            # shrink not supported
  model:
    args: false           # Don't overwrite args, because NMT args are not compatible here
    ensemble: 1           # how many checkpoints of parent to ensemble, to obtain initial state
optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1
  trainable: # trainable parameter
    # use either include or exclude but not both
    include: # only include these and exclude everything else not listed here
      - compressor
      - classifier
      - encoder:0

schedule:
  name: noam
  args:
    constant: 2
    warmup: 8000
    model_dim: 128

criterion:
  name: cross_entropy

prep:
  max_types: 4000
  pieces: bpe
  codec_lib: nlcodec
  shared_vocab: false
  src_len: 100
  tgt_len: 100
  train_src: tests/test-data/dbpedia/train.text
  train_tgt: tests/test-data/dbpedia/train.label
  truncate: true
  valid_src: tests/test-data/dbpedia/valid.text
  valid_tgt: tests/test-data/dbpedia/valid.label
tester:
  suite:
    valid:
    - tests/test-data/dbpedia/valid.text
    - tests/test-data/dbpedia/valid.label
  ensemble: 2
  batch_size: 6000
  max_len: 256

trainer:
  batch_size: 1024
  check_point: 250
  keep_models: 10
  steps: 3000
  sort_by: random
updated_at: '2021-07-28T16:10:24.858629'
