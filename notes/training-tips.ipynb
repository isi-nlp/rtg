{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting that last BLEU point\n",
    "**By changing the way dataset is batched**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural MT models are trained on a batch of sentence pairs. \n",
    "- The way batches are made (and submitted to optimizer's loss function) is often overlooked (but not anymore after this!).\n",
    "- Usually we are adviced to make random batches of sentences\n",
    "- But, we have to make deal with practical problems:\n",
    "  - GPUs have finite RAM (currently, about 11GB)\n",
    "  - Sequences are of unequal length\n",
    "  - Sequences can be extremely long or short word translations found in dictionaries\n",
    "- Before closely studying transformer model in tensor2tensor:\n",
    " - `batch_size` used to be number of sentences\n",
    " - We either filter away long sequences or truncate them to a specified `max_len`\n",
    " - We made truly random batches! Always! Shuffle the entire dataset --> start slicing it as batches as we read \n",
    "- After closely studying the transformer model in tensor2tensor\n",
    " - `batch_size` is number of tokens on the target side. \n",
    " - We dont need to filter the long sequences or truncate them as long as `max_len <= batch_size`\n",
    " - Truly random batches ? Nope. We disrupt the randomness by grouping similar length sentences into batches. Randomness is preserved at the level of batches but inside each batch the sentences have similar length.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the tensor2tensor style batching is good ?\n",
    "\n",
    "Loss of each batch is computed per target token on the target (and then normalized). Thus each token on the target side provides a supervision signal to the optimizer.\n",
    "We want more of those to get better supervision ==> More tokens in the batch the better.\n",
    "\n",
    "But in practice, we deal with unequal length sequences so we add padding at then end and mask them during the the loss calculation. Every padded token is therefore a waste of computation, so we have to minimize them. And also, it is a loss of opportunity: if it was a useful token, it could have provided a supervision signal. \n",
    "\n",
    "If we make batches by counting the number of sentences, some of them will be full, but many of them will be shorter. We could reduce the padding by sorting by length. But shorter sentences thus yield very little signal since they have few tokens. \n",
    "\n",
    "- Instead of batching by sentence count, we shall batch by token count on the target side.\n",
    "That gaurantee that each batch's loss is over approximately same number of tokens (maybe helpful to the optimizer).\n",
    "\n",
    "- We can minimize the wasted computation by minimizing the padded tokens. How? By making batches such that sentences have same number of target tokens.\n",
    "\n",
    "However, the randomness and approximate length grouping are somewhat contradictory. i.e. \n",
    "- if we go for truly random order of dataset (by shuffling all sentences), we dont get batches of approximately same length.\n",
    "- if we go for same length grouping, we disrupt the randomness. \n",
    "\n",
    "So, what we should do:\n",
    "- we make batches of similar lengths ahead of time\n",
    "- we shuffle the batches, not the idividual examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
