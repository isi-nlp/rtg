# this is a mini model for testing. See transformer.base or transformer.big
model_args:
  src_vocab: 466
  tgt_vocab: 466
  enc_layers: 1
  dec_layers: 2
  hid_size: 128
  ff_size: 256
  n_heads: 2
  attn_bias: true
  attn_dropout: 0.1
  dropout: 0.2
  activation: relu
  tied_emb: three-way
model_type: tfmnmt
optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1
    weight_decay: 0

schedule:
  name: noam
  args:
    constant: 2
    warmup: 4000
    model_dim: 512

criterion:
  name: smooth_kld
  args:
    label_smoothing: 0.1

prep:
  codec_lib: nlcodec
  max_types: 500
  pieces: bpe
  shared_vocab: true
  src_len: 128
  tgt_len: 128
  train_src: experiments/sample-data/sampl.test.fr.tok
  train_tgt: experiments/sample-data/sampl.test.en.tok
  truncate: false
  valid_src: experiments/sample-data/sampl.valid.fr.tok
  valid_tgt: experiments/sample-data/sampl.valid.en.tok
  valid_tgt_raw: experiments/sample-data/sampl.valid.en.tok
  mono_src: []
  mono_tgt: []
  max_part_size: 500_000   # maximum recs in each part of db; TODO: set it to 1_000_000 or more

spark: # add this block to enable spark backend
  spark.master: local[2]  # TODO: change this; give more CPUs
  spark.app.name: RTG NMT on Spark
  spark.driver.memory: 3g  #TODO: change this; give more RAM
  spark.serializer: org.apache.spark.serializer.KryoSerializer
  #spark.local.dir:
  #key1: value1    # any other spark configs you want to control
tester:
  decoder:
    beam_size: 4
    batch_size: 12000  # this is for 1 beam; effective_batch_size = batch_size / beam_size
    lp_alpha: 0.0     # length penalty
    ensemble: 5
    max_len: 50
  suite:
    valid:
    - experiments/sample-data/sampl.valid.fr.tok
    - experiments/sample-data/sampl.valid.en       # reference, unmodified -- not tokenized
trainer:
  #init_args:
  # chunk_size: 10
  batch_size: 512
  check_point: 200
  keep_models: 10
  steps: 1000
updated_at: '2020-08-02T20:51:09.002385'
seed: 12345
