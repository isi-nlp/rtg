# this is a mini model for testing. See transformer.base or transformer.big
model_args:
  model_id: pytorch/fairseq:xlmr.base  # torch hub model id
  init: ['src_in_emb', 'tgt_in_emb', 'tgt_out_emb', 'enc_layers', 'dec_layers', 'generator_dense']
  #alternatively, init: ['all']   ; listed out all in the above to have finer control
model_type: robertamt
optim:
  args:
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-09
    label_smoothing: 0.1
    lr: 0.1
    warmup_steps: 4000
  name: ADAM
prep:
  codec_lib: pretrainmatch
  pieces: pytorch/fairseq:xlmr.base   # match it with  model_id
  max_types: -1
  shared_vocab: true
  src_len: 128
  tgt_len: 128
  train_src: experiments/sample-data/sampl.test.fr.tok
  train_tgt: experiments/sample-data/sampl.test.en.tok
  truncate: false
  valid_src: experiments/sample-data/sampl.valid.fr.tok
  valid_tgt: experiments/sample-data/sampl.valid.en.tok
  mono_src: []
  mono_tgt: []
tester:
  decoder:
    beam_size: 4
    batch_size: 12000  # this is for 1 beam; effective_batch_size = batch_size / beam_size
    lp_alpha: 0.0     # length penalty
  suit:
    valid:
      - experiments/sample-data/sampl.valid.fr.tok
      - experiments/sample-data/sampl.valid.en     # reference, unmodified -- not tokenized
    test:
      - experiments/sample-data/sampl.test.fr.tok
      - experiments/sample-data/sampl.test.en     # reference, unmodified -- not tokenized-
trainer:
  init_args:
    chunk_size: 1
  batch_size: 120
  check_point: 200
  keep_models: 10
  steps: 400
updated_at: '2019-03-09T21:15:33.707183'
seed: 12345