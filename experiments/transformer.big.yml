model_args:
  ff_size: 4096
  hid_size: 1024
  n_heads: 16
  attn_dropout: 0.1
  dropout: 0.3
  enc_layers: 6
  dec_layers: 6
  src_vocab: 8000
  tgt_vocab: 8000
  tied_emb: three-way
model_type: transformer-nmt

optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1

schedule:
  name: noam
  args:
    constant: 2
    warmup: 8000
    model_dim: 1024

criterion:
  name: smooth_kld    #options "cross_entropy", "smooth_kld", "binary_cross_entropy", "triplet_loss"
  args:
    label_smoothing: 0.1

prep:
  max_types: 8000
  pieces: bpe
  shared_vocab: true
  src_len: 256
  tgt_len: 256
  train_src: data/train.src
  train_tgt: data/train.tgt
  truncate: true
  valid_src: data/valid.src
  valid_tgt: data/valid.tgt
  mono_src: []
  mono_tgt: []
tester:
  decoder:
    tune:
      trials: 6
      tune_src: data/valid.src
      tune_ref: data/valid.tgt
      beam_size: [1, 4, 8]
      ensemble: [1, 5, 10]
      lp_alpha: [0.0, 0.4, 0.6, 1.0]
      suggested:
        - 1, 1, 0.0
        - 4, 1, 0.0
        - 4, 1, 0.6
        - 1, 5, 0.0
        - 4, 5, 0.0
        - 4, 5, 0.6
        - 1, 10, 0.0
        - 4, 10, 0.0
        - 4, 10, 0.6
  suit:
    valid:
    - data/valid.src
    - data/valid.tgt
trainer:
  init_args:
    chunk_size: 10
    grad_accum: 1    # How many batches to accumulate gradients over
  batch_size: 2600
  check_point: 1000
  keep_models: 10
  valid_full_decode: false     # true = validations runs greedy decode (slow); false=teacher forcing (fast)
  steps: 200000
updated_at: '2019-03-09T21:15:33.707183'
seed: 12345