model_type: transformer-nmt
model_args:
  ff_size: 512
  hid_size: 256
  n_heads: 4
  attn_dropout: 0.1
  dropout: 0.2
  enc_layers: 3
  dec_layers: 3
  src_vocab: 8000
  tgt_vocab: 8000
  tied_emb: three-way

optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1

schedule:
  name: noam
  args:
    constant: 2
    warmup: 4000
    model_dim: 256

criterion:
  name: kl_divergence
  args:
    reduction: macro
    label_smoothing: 0.1

prep:
  codec_lib: nlcodec
  max_types: 500
  pieces: bpe
  shared_vocab: true
  src_len: 128
  tgt_len: 128
  train_src: experiments/sample-data/sampl.train.fr.tok
  train_tgt: experiments/sample-data/sampl.train.en.tok
  truncate: true
  valid_src: experiments/sample-data/sampl.valid.fr.tok
  valid_tgt: experiments/sample-data/sampl.valid.en.tok
  valid_tgt_raw: experiments/sample-data/sampl.valid.en
  mono_src: []
  mono_tgt: []
tester:
  suite:
    valid:
    - experiments/sample-data/sampl.valid.fr.tok
    - experiments/sample-data/sampl.valid.en

trainer:
  init_args:
    #chunk_size: 10
    grad_accum: 1    # How many batches to accumulate gradients over
  batch_size: 1024
  check_point: 400
  keep_models: 10
  steps: 2000
updated_at: '2019-03-09T21:15:33.707183'
seed: 12345