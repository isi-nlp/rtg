model_args:
  src_lang: FRA
  tgt_lang: ENG
  n_layers: 2
  emb_size: 300
  hid_size: 300
  src_vocab: 8000
  tgt_vocab: 8000
  attention: general
  tied_emb: three-way
model_type: seq2seq
optim:
  name: ADAM
  warmup_steps: 2000
trainer_args:
  steps: 128000
  check_point: 1000
  keep_models: 10
prep:
  mono_src: []
  mono_tgt: []
  src_len: 100
  text_pieces: false
  tgt_len: 100
  train_src: data/train.src
  train_tgt: data/train.tgt
  truncate: true
  valid_src: data/valid.src
  valid_tgt: data/valid.tgt
  shared_vocab: true
  max_types: 8000
  pieces: unigram
  finetune_src: data/finetune.src
  finetune_tgt: data/finetune.tgt
src_lang: FRA
tgt_lang: ENG
updated_at: '2018-07-25T15:50:52.578881'
