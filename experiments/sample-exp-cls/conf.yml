# run scripts/get-data/get-cls-data.sh to download data

model_args:
  src_vocab: 312
  tgt_vocab: 6
  enc_layers: 1
  hid_size: 128
  ff_size: 256
  n_heads: 2
  attn_bias: true
  attn_dropout: 0.1
  dropout: 0.2
  activation: relu
  cls_repr: prepend
model_type: tfmcls
optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    lr: 0.1

schedule:
  name: noam
  args:
    constant: 2
    warmup: 4000
    model_dim: 128

criterion:
  name: smooth_kld
  args:
    label_smoothing: 0.1

prep:
  max_types: 500
  codec_lib: nlcodec
  pieces: bpe
  src_len: 128
  tgt_len: 1
  train_src: data/trec/train.text
  train_tgt: data/trec/train.coarse
  truncate: true
  valid_src: data/trec/valid.text
  valid_tgt: data/trec/valid.coarse
  #valid_tgt_raw: experiments/sample-data/sampl.valid.en
tester:
  decoder:
    beam_size: 2
    batch_size: 12000  # this is for 1 beam; effective_batch_size = batch_size / beam_size
    lp_alpha: 0.0     # length penalty
    ensemble: 5
    max_len: 50
  suite:
    valid:
    - data/trec/valid.text
    - data/trec/valid.coarse
    test:
    - data/trec/test.text
    - data/trec/test.coarse
trainer:
  batch_size: [1024, 64]
  check_point: 500
  keep_models: 5
  steps: 20000
updated_at: '2023-03-03T05:26:15.365295'
seed: 12345
rtg_version:
  previous:
  last_worked: 0.7.3-dev
